\documentclass{stylesheet}

\usepackage{color}
\usepackage{url}
\usepackage{enumerate}
\usepackage{verbatim}
\usepackage[english]{babel}%spell check
\usepackage[colorinlistoftodos]{todonotes}%todo
\usepackage{etoolbox}%Remove the ugly copyright from the template :-)
\makeatletter%Remove the ugly copyright from the template :-)
\patchcmd{\maketitle}{\@copyrightspace}{}{}{}%Remove the ugly copyright from the template :-)
\makeatother%Remove the ugly copyright from the template :-)

\usepackage[nameinlink,capitalise]{cleveref}

\begin{document}
\title{Cloud Computing}

\numberofauthors{5} 
\author{
% 1st. author
\alignauthor
Christos Froussios\\
	\affaddr{4322754}\\
	\email{C.Froussios @student.tudelft.nl}
% 2nd. author
\alignauthor
Richard van Heest\\
	\affaddr{4086570}\\
	\email{A.W.M.vanHeest @student.tudelft.nl}
\and
% Alex
\alignauthor
Alexandru Iosup\\
	\affaddr{Course instructor}
	\email{A.Iosup@tudelft.nl}
% Dick
\alignauthor
Dick Epema\\
	\affaddr{Course instructor}
	\email{D.H.J.Epema@tudelft.nl}
% Alexey
\alignauthor
Alexey Ilyushkin\\
	\affaddr{Teaching Assistant}
	\email{A.S.Ilyushkin@tudelft.nl}
}

\maketitle

\begin{abstract}
In this report we describe our findings of 1 month of research in the area of cloud computing. We build a small prototype of a IaaS-based application that does image processing using the Amazon Web Services (AWS) EC2 cloud platform. The main features of this prototype are automation, scalability, load balancing, reliability and monitoring. We measured the performance of the system by applying metrics like \textcolor{red}{TODO \ldots}
\end{abstract}

\section{Introduction}
\textcolor{red}{+ title + abstract (150 words) = 1 page}

Cloud computing has gained an increasing interest in the last couple of years. In contrast to the era of grid computing, not only the academic world is interested in this new way of performing large scale computations, but also companies do so. The most logic explanation for this shift of interest is that computer grids are expensive to invest in for a company which may not use the required hardware constantly, whereas in cloud computing that company would lease the required machines, scale up or down if appropriate and only get charged for the hours between leasing and releasing.

We distinguish three types of cloud computing:
\begin{description}
	\item[Software as a Service (SaaS)] applications such as Google Drive, Dropbox, Gmail, Yahoo mail and Facebook.
	\item[Platform as a Service (PaaS)] the computing platforms which typically include operating systems, programming language execution environments, databases and web servers. Examples of this are Apache Hadoop, Google App Engine, Windows Azure and Amazon Web Services Elastic Beanstalk.
	\item[Infrastructure as a Service (IaaS)] the computing infrastructure, physical or (more often) virtual machines and other resources like virtual-machine disk image libraries, block and file-based storage, firewalls, load balancers, IP addresses and virtual local area networks. The most prominent products in this category are Windows Azure, Google Compute Engine and Amazon EC2.
\end{description}

In this report we will focus on IaaS cloud computing especially, as requested to investigate by WantCloud BV. We will look into the advantages and disadvantages of IaaS by implementing a small prototype for a potential future system. The main features of this prototype can be summarized as:
\begin{description}
	\item[Automation] working as much as possible independently from any human interaction
	\item[Elasticity (auto-scaling)] leasing and releasing machines from a resource pool as workloads change over time
	\item[Performance (load balancing)] allocating workloads to machines from the resource pool in such a way that the machines are used as effective as possible.
	\item[Reliability] building in a fair amount of fault tolerance
	\item[Monitoring] observing and recording the status of the system as well as measuring the performance of the system and its components by applying various metrics.
	\item[TODO] \textbf{\textit{\underline{additional features???}}}
\end{description}

The prototype to be implemented will be an application that receives pictures from an external source (for example a web form where users can submit their pictures), performs some operations on them and returns the processed pictures. These operations may vary from lightweight operations such as a flip of the picture to applying a Fourier transformation.

The application is set up in such a way that all pictures are received at a head node, which allocates them to one of the worker node in its resource pool and leases new machines or releases idle machines when appropriate. For fault tolerance reasons, the allocated picture is temporarily stored on the head node until its processed counterpart is received back in the head node, such that in the case of a failing worker node, the picture can be reallocated.

\textcolor{red}{TODO allocation policies, lease/release policies, communication, monitoring, head-node-never-fails-assumption?, \ldots}

This report is structured as follows: first we will discuss the background of the application in \cref{sec:background}. What does it do in detail? What are its requirements and features? This is followed by the system design in \cref{sec:design}, where we focus on the internal workings of the application as well as several policies for both the processes of allocation and leasing/releasing nodes. We evaluate the application in \cref{sec:experiments} by applying several metrics. Finally we close the report with a discussion (\cref{sec:discussion}) and the conclusion (\cref{sec:conclusion}). In \cref{app:time} an overview of spent time is provided.

\textcolor{red}{TODO Problem description, existing systems and/or tools, system to be implemented, structure of the rest of the paper}

\section{Background of the application}
\label{sec:background}
\textcolor{red}{0.5 page: describe application (1 paragraph) + requirements (1-3 paragraphs)}

The application we build over the course of several weeks is a prototype of a more generic cloud centered solution where tasks come in at the head node and are distributed over a number of worker nodes. In this particular application we choose to take image processing as the workload. Images that are received by an arbitrary worker are modified by applying a number of operations, combining them afterwards. Notice that this approach has a high potential for the task being split into subtasks and make it into a MapReduce process. However, to make things not overly complicated in this prototype, we consider one image as one \textit{undividable} task, which is executed on a single node.

The requirements of our application can be split up into 5 tiers. First of all, the application needs to run without human intervention as much as possible. We fulfilled this requirement by only having to start the head node application and providing images over the course of the runtime. Besides these 2 acts, the head node automatically decides when to lease and release worker nodes, to which worker an incoming task is sent and to where a completed task needs to be returned. On the other hand, the worker nodes automatically receive tasks, process them as soon as they possible can and make sure they send back the result to the head node.

A second tier holds the elasticity (or scalability) of the system, which is determined by the head node. Elasticity in the context of cloud computing describes to which degree the system is able to grow and shrink its number of resources. In this case, these resources are the worker nodes, which can be leased or released. When the system gets overloaded with tasks, it might be a good idea to lease more workers. On the other hand, when there are hardly any tasks in the system, it is useless to have a large number of idle workers, since they are being payed for all that time. Ideally you want to predict when the system will have a peak moment in the number of tasks and when it will get more quiet. For this, machine learning is a nice path to follow. This however is not part of this project.

Besides elasticity, load balancing is another topic of interest in cloud computing. When the head node receives a task, it needs to decide where it should be executed. Does it prefer certain workers, will it be assigned randomly or is there a better way to schedule a task. In this the head node might look at the size of the task, the length of the queue of each available worker or any other measurable property of the task and the workers.

The fourth tier is the reliability of the system. As the application is mostly autonomous, it also needs to be able to recover itself from failure. A failure might be losing connection between the head node and one or more workers, crashing the head or a worker, IO failure, etc. For this project we make the assumption that the head node never crashes and \textcolor{red}{complete this when we implemented this}.

Finally, a system isn't a good system without a monitoring module. To make decisions in the other tiers (especially elasticity and load balancing), we need to supply it with proper data, for example the length of each worker's queue, the number of incoming tasks in the system, etc. Also for the research part of this project the monitoring module is used. This is used to gain the results in \cref{sec:experiments}.

\section{System Design}
\label{sec:design}
\textcolor{red}{1.5 pages}

The application we present here can be seen as a prototype for a typical cloud centric workflow, where a user sends workloads to a head node, which distributes it over one or more workers (depending on whether the workloads are splittable into subtasks). Once the workload is processed, a result is send back to the head node, which forwards this result to the appropriate user. Based on the workloads, the head node needs to decide to which worker a workload is allocated and whether or not to lease or release workers, for which it uses certain policies.

In this section we will discuss the system's architecture, workflow, communication between client, head node and workers, as well as the policies that are used in scaling and load balancing. \textcolor{red}{Furthermore we will describe some additional features that are in the system.}

\subsection{Resource Management}
\label{subsec:resourceManagement}
\textcolor{red}{Description of the design of the system + provisioning, allocation, reliability and monitoring}

In order to understand the system's architecture and workflow, let's follow a workload as it works its way through the system. We refer to \cref{fig:headnode} and \cref{fig:worker} for an overview of the architecture.

The user who sends this workload will sign in to the head node via a socket connection. This connection request comes in at the {\verb ClientReception }, which creates a {\verb ClientHandle } for workloads to be received. From this point on, the user interacts directly with its own {\verb ClientHandle } in the head node. When this receives a workload, it makes a local copy of the workload and adds it to the {\verb TaskQueue }. This is where all workloads come together while waiting to be processed. It should be noted that (for reasons explained later) the {\verb TaskQueue } is a double-ended queue, meaning that tasks not only can be pushed at the end of the queue but also at the beginning.

\begin{figure*}
	\centering
	\includegraphics[width=\textwidth]{Architecture_head.png}
	\caption{Architecture in the head node}
	\label{fig:headnode}
\end{figure*}

\begin{figure*}
	\centering
	\includegraphics[width=0.8 \linewidth]{Architecture_worker.png}
	\caption{Architecture in a worker}
	\label{fig:worker}
\end{figure*}

The {\verb Monitor } continuously samples the current state of the head node. For this reason, it also knows that the workload is queued. Based on the total workload in the queue (and other metrics and policies that are explained later), it decides whether or not to lease extra workers. When extra workers are needed, the {\verb Monitor } makes a call to the Cloud Service to request a new machine.

The {\verb WorkerReception } waits for this machine to contact the head node. Once the socket connection is established, a {\verb WorkerHandle } is created and stored in the head node, via which all future communication with the newly leased machine will take place. The first communication is the worker who introduces itself and tell how many cores it has. This information is stored in the {\verb WorkerHandle } for later use. Once this formal procedure is over, the new worker will wait for the first workloads to come in.

Besides coordinating the leasing of workers, the {\verb Monitor } is also responsible for cleaning up workers that are not needed anymore. This is done based on metrics and policies that will be discussed later. When it decides to release a worker, the corresponding {\verb WorkerHandle } will be flagged as being set for decommision. After that, no more tasks can be scheduled on that worker; it will only finish its currently scheduled tasks, after which is is released.

The {\verb Scheduler } takes workloads out of the {\verb TaskQueue } and decides on which workers they will be processed. This subsystem takes all tasks out of the queue and collects all eligible workers (the ones that are not flagged for decommision). Based on a predefined policy, an allocation plan is made where tasks are assigned to certain workers. It is possible in some policies that not all tasks can be scheduled at once. For that reason, not only the allocation plan is returned by the policies, but also a list of not yet scheduled tasks. The latter will be put back on top of the queue, so that they are the first to be scheduled again next time. The tasks that were in the allocation plan will get assigned to the appropriate workers by sending their workloads over the previously established socket connection. In the case the connection failed in the meantime, the task is put back in the queue as well to be rescheduled.

When a task is allocated to a certain worker, its workload will be send over there via the socket connection. At the worker side, it is received by the {\verb Input } subsystem, which puts it in the {\verb InputQueue } for it to be processed. When the {\verb Processing } subsystem is finished with the previous task, it will take the workload out of the queue and performs the actual processing. After that is finished, the result is put into the {\verb OutputQueue }, from where it is send back to the head node by the {\verb Output } subsystem.

At the head node, the result is received over the socket  by the {\verb WorkerHandle } which removes the corresponding task from the list of currently processed jobs and adds the result to the {\verb ProcessedQueue }. There it waits until the {\verb Responder } takes it out and sends it back to the user.

\subsection{Architectural improvements}
The architecture described above is what the application currently looks like. However, this does not fully satisfy the redundancy requirement. This is due to this project's deadline. In the following we describe what to change in and add to the architecture to fully support this requirement.

\textcolor{red}{@Chris, describe what happens when a worker fails during execution. How is this detected and how is the head node responding to that? Also mention that we assume the head node to not fail.}

\subsection{System policies}
\label{subsec:policies}
\textcolor{red}{Description of the supported (future work) and used policies.}

When it comes to scheduling tasks or leasing and releasing workers, the approach can influence the performance of the whole system. In this section we discuss the different policies we created for both scheduling, leasing and releasing.

\subsubsection*{Scheduling policies}
Every scheduling policy implementation receives a list of tasks to be scheduled as well as a list of eligible workers and has to return both an allocation plan and a list of unscheduled tasks.

\ldots

\subsubsection*{Leasing and releasing policies}
\textcolor{red}{@Chris, can you write this?}


\subsection{Additional System Features}
\label{subsec:additionalFeatures}
\textcolor{red}{@Chris, Additional features: can you check which ones we have to write about?}
\ldots
\subsubsection{Feature 1}
\ldots
\subsubsection{Feature 2}
\ldots

\section{Experiments}
\label{sec:experiments}
\textcolor{red}{1.5 pages}
\subsection{Experimental setup}
\label{subsec:setup}
\textcolor{red}{Working environment (EC2), general workload and monitoring tools and other libraries}

\subsection{Experimental results}
\label{subsec:results}
\textcolor{red}{Description of the experiments conducted to analyze each system feature, analyze them and describe the workload, present the operation of the system and analyze the results. See assignment}

\section{Discussion}
\label{sec:discussion}
\textcolor{red}{1 page - main findings, tradeoffs inherent in the design of cloud-computing-based applications.}

\section{Conclusion}
\label{sec:conclusion}
\textcolor{red}{\ldots}

\bibliographystyle{abbrv}
\bibliography{references}

\appendix
\section{Time sheet}
\label{app:time}
\textcolor{red}{table with time spend}
\end{document}